{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# #stable diffusion\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# #stable diffusion between two prompts\n",
    "from stable_diffusion_videos import StableDiffusionWalkPipeline\n",
    "\n",
    "\n",
    "#whisper and a tool to download Youtube videos\n",
    "import whisper\n",
    "import pytube\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Object detection is the task of locating and classifying objects in an image or video.\n",
    "\n",
    "\n",
    "- YOLO is a state-of-art object detection library and completely open-source.\n",
    "\n",
    "\n",
    "- Click [here](https://github.com/ultralytics/yolov5) to learn more about it. \n",
    "\n",
    "\n",
    "- You can easily deploy YOLO for your own object detection problems. [Here](https://www.youtube.com/watch?v=tFNJGim3FXw) is an amazing tutorial for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s',force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture(\"nascar.mp4\")\n",
    "while cap.isOpened():\n",
    "    #grab the frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    #detect the objects\n",
    "    if frame is not None:\n",
    "        results = model(frame)\n",
    "    \n",
    "        cv2.imshow('YOLO', np.squeeze(results.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='best_model.pt', force_reload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the detection\n",
    "img = 'dataset/test/images/510x.jpg'\n",
    "results = model(img)\n",
    "\n",
    "#plot the detection\n",
    "img = Image.fromarray(np.squeeze(results.render()))\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('dataset/myvideo.mp4')\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if frame is not None:\n",
    "        # Make detections \n",
    "        results = model(frame)\n",
    "        cv2.imshow('YOLO', np.squeeze(results.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Images from Text: Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text-to-Image deep learning architecture. The model was trained on 2.6 billion images with approximate cost of **$600,000**\n",
    "- If you want to try it in your browser, visit [HuggingFace](https://huggingface.co/spaces/stabilityai/stable-diffusion)\n",
    "- If you hit login issue, uncomment and run the code below.\n",
    "- Be creative and try out different long, descriptive prompts. For example, you get inspiration from [here](https://mpost.io/best-100-stable-diffusion-prompts-the-most-beautiful-ai-text-to-image-prompts/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id,revision=\"fp16\", \n",
    "                                               torch_dtype=torch.float16,\n",
    "                                               use_auth_token=True)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter a prompt\n",
    "prompt = \"ultrarealistic, (native american old man ) portrait, cinematic lighting,\\\n",
    "award winning photo, no color, 80mm lense –beta –upbeta –upbeta\"\n",
    "\n",
    "pipe(prompt).images[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"the living room of a cozy wooden house with a fireplace, wallpaper, warm, digital art. \\\n",
    "art by james gurney and larry elmore\"\n",
    "pipe(prompt).images[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promt = \"kneeling cat knight, portrait, finely detailed armor,\\ \n",
    "intricate design, silver, silk, cinematic lighting, 4k\"\n",
    "pipe(prompt).images[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Artwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Things are getting interesting. If we can generate images from prompts, why not to generate images between two different prompts just like interpolation. \n",
    "\n",
    "\n",
    "- Enter two prompts and check the video in *stable_diffusion_videos* folder. Increase *\"num_images\"* to get more and more interesting prompt evolution\n",
    "\n",
    "\n",
    "- Check out the videos I already generated in \"stable_diffusion_videos\" folder. Try out different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = 'old rusty 18th century building in desert'\n",
    "prompt2 = 'a futuristic building, vivid colors, 4K'\n",
    "num_images = 60\n",
    "\n",
    "pipeline = StableDiffusionWalkPipeline.from_pretrained(model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    revision=\"fp16\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipeline.enable_attention_slicing()\n",
    "video_path = pipeline.walk(\n",
    "    prompts=[prompt1,prompt2],\n",
    "    seeds=[42, 1337],\n",
    "    num_interpolation_steps = num_images,  #number of images to generate in betweeen propmts\n",
    "    height=512,                            # use multiples of 64 if > 512. Multiples of 8 if < 512.\n",
    "    width=512,                             # use multiples of 64 if > 512. Multiples of 8 if < 512.\n",
    "    output_dir='stable_diffusion_videos',  # Where images/videos will be saved\n",
    "    name='building_test',                  # Subdirectory of output_dir where images/videos will be saved\n",
    "    guidance_scale = 5,                    # Higher adheres to prompt more, lower lets model take the wheel\n",
    "    num_inference_steps=50,                # Number of diffusion steps per image generated. 50 is good default\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model is by OpenAI and trained on 680,000 hours of 680 000 multilingual audio. If you sleep 8 hours a day and live 80 years, you can talk/listen at most 16*365*80=467200 hours.\n",
    "- Check the repo [here](https://github.com/openai/whisper)\n",
    "- Put any recording in the current folder and change the filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"file.m4a\"\n",
    "model = whisper.load_model(\"small\")\n",
    "result = model.transcribe(filename)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Transcribe Youtube Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download and transcribe any Youtube videos. Just past the link below and see what happens. Let's try out a fairly long video about T-cells and B-cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('eOStU5kaCpk', width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download and get the audio\n",
    "video = 'https://www.youtube.com/watch?v=eOStU5kaCpk'\n",
    "data = pytube.YouTube(video)\n",
    "data_name = data.title+'.mp4'\n",
    "\n",
    "# Convert to audio file\n",
    "audio = data.streams.get_audio_only()\n",
    "audio.download()\n",
    "\n",
    "#let the Whisper do the rest\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(data_name)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Hugging Face](https://huggingface.co/) is an open-source community that provides state-of-the-art deep learning models, mostly based on Transformers, to developers and researchers.\n",
    "\n",
    "\n",
    "- Hugging Face also offers a cloud-based platform called Hugging Face Hub that allows users to share, train, and deploy deep learning models. To learn how to use it here is their [official tutorial](https://youtube.com/playlist?list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o)\n",
    "\n",
    "\n",
    "\n",
    "- Almost every task in HuggingFace is wrapped up in the pipeline function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")\n",
    "summarizer(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_translator = pipeline(task=\"translation\",model=\"Helsinki-NLP/opus-mt-tc-big-tr-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"İstanbul Valiliği’nin açıklamasında 17 Ağustos 1999 Marmara depremi \\\n",
    "nedeniyle 2006’da kurulan İstanbul Proje Koordinasyon Birimi’nin (İPKB) projesi kapsamındaki çalışamalarda \\\n",
    "93 riskli okuldan 76 okulun yıkılıp yeniden yapılması,\\\n",
    "17’sinin de güçlendirilmesi kararlaştırıldı.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_text = pipeline(task=\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_image = Image.open(\"babies.jpg\")\n",
    "baby_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_text(baby_image)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_image = Image.open(\"cars_image.jpg\")\n",
    "car_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_text(car_image)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try out the exercises in HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transcribe the conversation between multiple people and process it.\n",
    "\n",
    "\n",
    "- Scrap the news from a given URL and summarize it.\n",
    "\n",
    "\n",
    "- Grab a Youtube video in a different language, transcribe, translate and summarize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generative Pretrained **Transformers**(GPT) is a deep learning model with over 175 billion. \n",
    "- ChatGPT was trained on 570GB of textual data.\n",
    "- It cost $50 million to train ChatGPT.\n",
    "- Visit [https://chat.openai.com/chat](https://chat.openai.com/chat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
